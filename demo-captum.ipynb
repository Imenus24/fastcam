{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the captum that includes `MultiscaleFastCam` functionality, please install a modified version by running the following commands: \n",
    "\n",
    "```\n",
    "$ git clone https://github.com/ryanchankh/captum\n",
    "$ cd captum\n",
    "$ git checkout develop\n",
    "$ pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from captum.attr import (\n",
    "    LayerAttribution,\n",
    "    LayerGradCam,\n",
    "    MultiscaleFastCam\n",
    ")\n",
    "\n",
    "from gradcam.utils import visualize_cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run FastCAM with SMOE Scale and Gamma Norm\n",
    "This notebook's version of fastcam is to combine multiple layers into one object, such that it examplifies the importance of multiscale saliency map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the image and create the appropriate image tensors\n",
    "- `raw_img`: image without transformation\n",
    "- `transformed_img`: image after transform\n",
    "- `transformed_imgs`: stack two `transform_img` together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_image_name = \"./images/ILSVRC2012_val_00049934.224x224.png\"\n",
    "cv_im = cv2.imread(input_image_name)\n",
    "cv_im = (cv2.cvtColor(cv_im, cv2.COLOR_BGR2RGB) / 255.0).astype(np.float32)\n",
    "inputs = transform(cv_im)\n",
    "inputs = inputs.unsqueeze(0)\n",
    "_, channels, in_height, in_width = inputs.size()\n",
    "\n",
    "raw_img = torch.tensor(cv_im.transpose(2, 0, 1), requires_grad=True).unsqueeze(0)\n",
    "transformed_img = inputs\n",
    "transformed_imgs = torch.cat([inputs, inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our ResNet50 PyTorch Module here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=True).to(device).eval()\n",
    "y_pred = model(transformed_img).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first intialize captum fastcam module by passing in:\n",
    "- `model`:  our ResNet50 Model\n",
    "- `layers`: the layers that we want to combine the saliency maps of\n",
    "- `norm`:   the choice of norm we apply after SMOE Scale\n",
    "\n",
    "Then by calling `attribute()` returns a tuple of attributions outputed by fastcam\n",
    "`attributes` has length of 5 and has different shapes based on the size of\n",
    "the intermediate layers chosen.\n",
    "\n",
    "Finally, `combine()` is a static method that returns the combined map using the differet\n",
    "scales of attributions. We can adjust the weight here for different scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcam = MultiscaleFastCam(model,\n",
    "                            layers=[model.relu,\n",
    "                                    model.layer1[2].relu,\n",
    "                                    model.layer2[3].relu,\n",
    "                                    model.layer3[5].relu,\n",
    "                                    model.layer4[2].relu],\n",
    "                            norm=\"gamma\")\n",
    "\n",
    "\n",
    "attributes = fastcam.attribute(transformed_imgs)\n",
    "\n",
    "\n",
    "combined_map, weighted_maps = fastcam.combine(attributes,\n",
    "                                              weights=[1.0 for _ in range(5)],\n",
    "                                              output_shape=(in_height, in_width),\n",
    "                                              relu_attribution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the combined saliency maps\n",
    "csmap = combined_map[0].numpy().transpose(1, 2, 0).squeeze()\n",
    "plt.imshow(csmap, cmap='gray')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot of Weighted saliency maps\n",
    "fig, ax = plt.subplots(ncols=5, figsize=(10, 4))\n",
    "[ax[i].imshow(wmap.numpy(), cmap='gray') for i, wmap in enumerate(weighted_maps[0])]\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine with GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of FastCam is that it allows use to combine saliency maps with GradCAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    _min, _max = x.min(), x.max()\n",
    "    return (x - _min).div(_max - _min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a GradCAM attribution module using Captum\n",
    "GradCAM works usually on the last layer, so we are going \n",
    "to pass in `model.layer4`, and get its attributions like \n",
    "we did for FastCAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam = LayerGradCam(model, model.layer4)\n",
    "attribute_gradcam = gradcam.attribute(raw_img, \n",
    "                                      target=y_pred,\n",
    "                                      relu_attributions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to interpolate such that the output shape is the \n",
    "same as that of FastCAM, and finally noramlize the attributions \n",
    "using for visualization using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_gradcam = LayerAttribution.interpolate(attribute_gradcam, \n",
    "                                                 (in_height, in_width),\n",
    "                                                 'bilinear')\n",
    "attribute_gradcam = normalize(attribute_gradcam)\n",
    "attribute_gradcam = attribute_gradcam.detach().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a GradCAM attribution module using Captum\n",
    "# GradCAM works usually on the last layer, so we are going \n",
    "# to pass in `model.layer4`, and get its attributions like \n",
    "# we did for FastCAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combing FastCAM with GradCAM allows us to create in-class maps\n",
    "and out-class maps. Then plotting the heatmap using \n",
    "`visualize_cam` from gradcam library for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inclass_map = csmap * attribute_gradcam\n",
    "outclass_map = csmap * (1 - attribute_gradcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inclass_map = csmap * attribute_gradcam\n",
    "outclass_map = csmap * (1 - attribute_gradcam)\n",
    "\n",
    "heatmap, result = visualize_cam(torch.tensor(inclass_map),\n",
    "                                raw_img,\n",
    "                                1)\n",
    "plt.imshow(result.detach().numpy().transpose(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
